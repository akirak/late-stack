---
title: Effect OGP Integration
language: en
draft: true
---

:::warning

Some part of this post has been generated by AI. It is reviewed by human, but
the content may not reflect the actual linguistic proficiency of the author.

:::

This document specifies an Open Graph Protocol (OGP) integration of this blog
system.

## Examples

Here's how the OGP integration works:

::link[https://effect.website]

::link[https://github.com/anthropics/claude-code]

## Purpose: Fetching OGP metadata at build time

Modern blogs are more than static pages—they are hubs that reference videos,
tweets, papers and other rich resources. Showing a preview card for each
external URL greatly improves user experience. Yet fetching metadata **at render
time** is slow and unreliable.

By fetching all metadata of referenced external contents **at build time** and
storing them locally, the application never performs additional network requests
for OGP data. This will significantly improve the user experience of any
frontend application.

## Architecture description

### 1. Bird’s-eye view

```d2
direction: right

Markdown["Markdown posts"] -> Pipeline[
  shape: component
  label: "Collections Pipeline"
]
Pipeline -> Extract[
  shape: note
  label: "extract external links"
]
Pipeline -> Cache[
  label: "OGP cache (SQLite)"
]
Pipeline -> Fetcher[
  shape: component
  label: "HTTP / HTML fetcher"
]
Fetcher -> Internet[
  shape: cloud
]
Fetcher -> Cache
Cache -> Pipeline
Pipeline -> PostJson[
  shape: database
  label: "Post JSON artefacts"
]
```

The cache and fetcher live only during the build. At runtime the blog simply
reads the pre-fetched metadata already embedded in each post.

### 2. Data model

Each record keeps just enough to draw a link card while remaining future-proof:

- canonical URL
- title, description, cover image, site name, content type
- timestamp of the last fetch
- “unknown tags” – a bag for provider-specific keys we don’t yet understand

Keeping a timestamp lets us apply a simple _time-to-live_ (TTL) strategy instead
of hard invalidation rules.

A metadata schema for each resource is defined in an Effect-TS schema.
It is decoded from a JSON object, loaded from a KVS cache.

### 3. OGP service

- get(url) → maybe metadata
- set(metadata)
- purgeOlderThan(ttl) → rows removed

OGP Fetcher

- fetch(url) → metadata _or_ typed error

Combining them gives **getOgMetadata(url, ttl)** that

1. looks in the store,
2. falls back to the fetcher if missing or stale,
3. writes the fresh copy back.

```d2
sequence: getOgMetadata
Client->OGPService: getOgMetadata(url)
OGPService->Cache: get(url)
Cache->OGPService: miss/stale
OGPService->Fetcher: fetch(url)
Fetcher->Internet: HTTP GET
Internet->Fetcher: HTML
Fetcher->OGPService: metadata
OGPService->Cache: set(metadata)
OGPService->Client: metadata
```

The call never throws – typed failures are propagated as explicit **Effect**
errors, so the pipeline can continue building even if a provider is down.

### 4. Storage layer

The key-value store is also defined as an Effect service. You can use a
different backend storage, e.g. native in-memory, relational database, Redis,
etc.

I will use SQLite because:

- single-file, zero-config, battle-tested
- native JSON column + indexes
- works the same on a developer laptop, CI runner and Deno Deploy

:::note

SQLite supports [dealing with JSON values](https://sqlite.org/json1.html).
Storing the metadata object as a JSON allows partial fetching and filtering,
unlike storing the entire object as a serialized string. This can entail
slight performance overhead, this will be convenient for listing particular
types of sources for the entire site, for example. Thus I will define the KVS to
have JSON object values.

:::

<!-- A 60-day retention policy keeps the file size in check. -->
<!-- older rows are purged once per full build. -->

<!--
Need Redis or Postgres?  Just implement the three Store functions and wire a new
layer – no changes required elsewhere.
 -->

### 5. Fetcher responsibilities

In the pipeline, all posts are rebuilt every time the development server starts
or production build is run. Thus it is important to minimise refetching, which
is generally much slower than accessing a local file system:

1. Follow up to 5 redirects, 10 s total timeout, 1 MiB body cap.
2. Parse _og:_ and _twitter:_ meta tags, then fall back to `<title>` /
   first `<img>`.
3. Resolve relative URLs against the final location.
4. Normalise & validate the result before handing it over.

All network, parsing and validation errors are mapped onto a concise error type
in Effect-TS. The caller can decide whether to retry later or silently omit the
card.

For parsing HTML, I will use `html-rewriter-wasm` npm package from Cloudflare.

### 6. Integration points

The service is only called at build time:

- A custom remark plugin for embedding links (via `::link` directive) calls
  **getOgMetadata** on generic external sources and replace the directives
  in-place. Link cards are only generated for block-level (i.e. container and
  leaf) directives. For text directives, only an icon and tooltip are added.

<!--
Runtime
(No runtime fetching – the client renders link previews solely with the metadata bundled inside the post JSON.)
 -->

### 7. Deployment & CI caching

- Local development: The database lives at `data/og.sqlite` (git-ignored).

- GitHub Actions: The same file is cached with `actions/cache`. the TTL logic
  makes the vast majority of builds _cache-only_, minimising network traffic and
  rate-limit risks.

- Production (Deno Deploy): the populated file is copied into `.output` during
  `pnpm build` – no runtime writes required.

<!--
### 8. Roadmap

* oEmbed fallbacks for Twitter, SoundCloud, etc.
* `pnpm og:purge` CLI to manually clear the cache.
* Scheduled GitHub Action refreshing stale rows weekly.
* Optional CDN layer for anonymous prod traffic.
 -->

## Credits

The initial idea was drafted with Gemini 2.5 Flash and then refined with OpenAI
o3. This document captures the refined, implementation-ready architecture while
keeping all code in the source tree DRY.
